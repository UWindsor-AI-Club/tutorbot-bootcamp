{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59894320",
   "metadata": {
    "papermill": {
     "duration": 0.012596,
     "end_time": "2024-08-23T19:00:51.320138",
     "exception": false,
     "start_time": "2024-08-23T19:00:51.307542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 2: Working with Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af4b8e",
   "metadata": {
    "papermill": {
     "duration": 0.011571,
     "end_time": "2024-08-23T19:01:09.163562",
     "exception": false,
     "start_time": "2024-08-23T19:01:09.151991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "These packages will be used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4196439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:09.189412Z",
     "iopub.status.busy": "2024-08-23T19:01:09.188971Z",
     "iopub.status.idle": "2024-08-23T19:01:12.733667Z",
     "shell.execute_reply": "2024-08-23T19:01:12.732388Z"
    },
    "papermill": {
     "duration": 3.561204,
     "end_time": "2024-08-23T19:01:12.736707",
     "exception": false,
     "start_time": "2024-08-23T19:01:09.175503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from importlib.metadata import version\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf6b04",
   "metadata": {
    "papermill": {
     "duration": 0.011594,
     "end_time": "2024-08-23T19:01:12.760677",
     "exception": false,
     "start_time": "2024-08-23T19:01:12.749083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Create tokenizer from a text file\n",
    "\n",
    "- Tokenize text: breaking text into smaller units, such as individual words and punctuation characters\n",
    "- Convert the text into vector of numbers (embeddings) so that LLMs work with them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2300fa7",
   "metadata": {
    "papermill": {
     "duration": 0.011632,
     "end_time": "2024-08-23T19:01:12.784599",
     "exception": false,
     "start_time": "2024-08-23T19:01:12.772967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will first take a look at the raw input text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a9c17b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:12.810395Z",
     "iopub.status.busy": "2024-08-23T19:01:12.809848Z",
     "iopub.status.idle": "2024-08-23T19:01:13.012170Z",
     "shell.execute_reply": "2024-08-23T19:01:13.010953Z"
    },
    "papermill": {
     "duration": 0.218325,
     "end_time": "2024-08-23T19:01:13.014802",
     "exception": false,
     "start_time": "2024-08-23T19:01:12.796477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    \n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebadced",
   "metadata": {
    "papermill": {
     "duration": 0.011663,
     "end_time": "2024-08-23T19:01:13.038656",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.026993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### a. Tokenize text\n",
    "Split the raw text by spaces and various types of punctuation, such as periods and question marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "682f1f07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:13.064768Z",
     "iopub.status.busy": "2024-08-23T19:01:13.064351Z",
     "iopub.status.idle": "2024-08-23T19:01:13.076371Z",
     "shell.execute_reply": "2024-08-23T19:01:13.075209Z"
    },
    "papermill": {
     "duration": 0.028474,
     "end_time": "2024-08-23T19:01:13.079391",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.050917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c6204",
   "metadata": {
    "papermill": {
     "duration": 0.011691,
     "end_time": "2024-08-23T19:01:13.103375",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.091684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### b. Build a vocabulary \n",
    "Collect all the unique tokens from raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77b963f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:13.129926Z",
     "iopub.status.busy": "2024-08-23T19:01:13.129123Z",
     "iopub.status.idle": "2024-08-23T19:01:13.138120Z",
     "shell.execute_reply": "2024-08-23T19:01:13.136961Z"
    },
    "papermill": {
     "duration": 0.025352,
     "end_time": "2024-08-23T19:01:13.141060",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.115708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some entries (tokens and their ids) in the voculary\n",
      "('yet', 1125)\n",
      "('you', 1126)\n",
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "Vocabulary size 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "print('Some entries (tokens and their ids) in the voculary')\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)\n",
    "print(f'Vocabulary size {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5789de",
   "metadata": {
    "papermill": {
     "duration": 0.012183,
     "end_time": "2024-08-23T19:01:13.165554",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.153371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### c. Adding special context tokens\n",
    "- |endoftext| to sperate documents\n",
    "- |unk| for tokens that dont exists in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa1a0622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:13.192164Z",
     "iopub.status.busy": "2024-08-23T19:01:13.191302Z",
     "iopub.status.idle": "2024-08-23T19:01:13.199340Z",
     "shell.execute_reply": "2024-08-23T19:01:13.198133Z"
    },
    "papermill": {
     "duration": 0.024157,
     "end_time": "2024-08-23T19:01:13.201986",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.177829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4ae3a2",
   "metadata": {
    "papermill": {
     "duration": 0.011921,
     "end_time": "2024-08-23T19:01:13.226368",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.214447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Example**: \n",
    "- Create **Simple Tokenizer** class to convert tokens to token ids (encode) and vice versa (decode). \n",
    "- However, we will use the tokenizer from **tiktoken** with larger vocabulary size (50K) later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69fc95b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:13.252878Z",
     "iopub.status.busy": "2024-08-23T19:01:13.252453Z",
     "iopub.status.idle": "2024-08-23T19:01:13.262053Z",
     "shell.execute_reply": "2024-08-23T19:01:13.260793Z"
    },
    "papermill": {
     "duration": 0.025758,
     "end_time": "2024-08-23T19:01:13.264518",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.238760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b0e973",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:13.290990Z",
     "iopub.status.busy": "2024-08-23T19:01:13.290547Z",
     "iopub.status.idle": "2024-08-23T19:01:13.297565Z",
     "shell.execute_reply": "2024-08-23T19:01:13.296355Z"
    },
    "papermill": {
     "duration": 0.023492,
     "end_time": "2024-08-23T19:01:13.300428",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.276936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998e74fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:13.328819Z",
     "iopub.status.busy": "2024-08-23T19:01:13.327753Z",
     "iopub.status.idle": "2024-08-23T19:01:13.337131Z",
     "shell.execute_reply": "2024-08-23T19:01:13.336009Z"
    },
    "papermill": {
     "duration": 0.026015,
     "end_time": "2024-08-23T19:01:13.339978",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.313963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6f37701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:13.367491Z",
     "iopub.status.busy": "2024-08-23T19:01:13.367044Z",
     "iopub.status.idle": "2024-08-23T19:01:13.375548Z",
     "shell.execute_reply": "2024-08-23T19:01:13.374366Z"
    },
    "papermill": {
     "duration": 0.025363,
     "end_time": "2024-08-23T19:01:13.378167",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.352804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8cd93",
   "metadata": {
    "papermill": {
     "duration": 0.012657,
     "end_time": "2024-08-23T19:01:13.403625",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.390968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5a4df",
   "metadata": {
    "papermill": {
     "duration": 0.012499,
     "end_time": "2024-08-23T19:01:13.429064",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.416565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f8510",
   "metadata": {
    "papermill": {
     "duration": 0.01251,
     "end_time": "2024-08-23T19:01:13.454431",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.441921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### a. An example of training samples\n",
    "\n",
    "We first read the-verdict.txt and will use this as our dataset for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b476bd11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:13.483117Z",
     "iopub.status.busy": "2024-08-23T19:01:13.482667Z",
     "iopub.status.idle": "2024-08-23T19:01:13.499911Z",
     "shell.execute_reply": "2024-08-23T19:01:13.498691Z"
    },
    "papermill": {
     "duration": 0.034828,
     "end_time": "2024-08-23T19:01:13.502525",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.467697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw text 4690\n",
      "in ----> a\n",
      "in a ----> villa\n",
      "in a villa ----> on\n",
      "in a villa on ----> the\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "context_size = 4\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(\"Length of raw text\", len(enc_text))\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8511a9",
   "metadata": {
    "papermill": {
     "duration": 0.0125,
     "end_time": "2024-08-23T19:01:13.528090",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.515590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We use a **sliding window approach**, changing the position by +1 (**stride** = 1):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f6326",
   "metadata": {
    "papermill": {
     "duration": 0.012485,
     "end_time": "2024-08-23T19:01:13.553558",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.541073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### b. Create Pytorch dataset and data loader.\n",
    "- dataset: to sample one sample.\n",
    "- dataloader: to sample a batch of samples from the dataset\n",
    "\n",
    "Note that in this section we will use the tokenizer from **tiktoken.get_encoding(\"gpt2\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6753290a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:13.581909Z",
     "iopub.status.busy": "2024-08-23T19:01:13.581465Z",
     "iopub.status.idle": "2024-08-23T19:01:13.593418Z",
     "shell.execute_reply": "2024-08-23T19:01:13.592363Z"
    },
    "papermill": {
     "duration": 0.029147,
     "end_time": "2024-08-23T19:01:13.596126",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.566979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321fb562",
   "metadata": {
    "papermill": {
     "duration": 0.012701,
     "end_time": "2024-08-23T19:01:13.622059",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.609358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can use dataloader to iterate through samples. We can see the **overlap** between the sequences because we used a stride of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcdd7840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:13.649906Z",
     "iopub.status.busy": "2024-08-23T19:01:13.649444Z",
     "iopub.status.idle": "2024-08-23T19:01:17.207245Z",
     "shell.execute_reply": "2024-08-23T19:01:17.206051Z"
    },
    "papermill": {
     "duration": 3.574843,
     "end_time": "2024-08-23T19:01:17.209942",
     "exception": false,
     "start_time": "2024-08-23T19:01:13.635099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch: [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "Second batch: [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(\"First batch:\", first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(\"Second batch:\", second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507f7fd",
   "metadata": {
    "papermill": {
     "duration": 0.012722,
     "end_time": "2024-08-23T19:01:17.235914",
     "exception": false,
     "start_time": "2024-08-23T19:01:17.223192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the following example, we used a batch size of 8 with a stride of 4. As a result:\n",
    "- The input, and output will have 8 vectors of token ids.\n",
    "- We do not see the **overlap** between the sequences, because the stride = 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c58bb3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:17.264556Z",
     "iopub.status.busy": "2024-08-23T19:01:17.263484Z",
     "iopub.status.idle": "2024-08-23T19:01:17.314451Z",
     "shell.execute_reply": "2024-08-23T19:01:17.313300Z"
    },
    "papermill": {
     "duration": 0.068496,
     "end_time": "2024-08-23T19:01:17.317656",
     "exception": false,
     "start_time": "2024-08-23T19:01:17.249160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0cbd44",
   "metadata": {
    "papermill": {
     "duration": 0.01259,
     "end_time": "2024-08-23T19:01:17.343640",
     "exception": false,
     "start_time": "2024-08-23T19:01:17.331050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Create token embedding and position embedding layers.\n",
    "\n",
    "### a. Token embedding\n",
    "* The data is already almost ready for an LLM\n",
    "* But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
    "* Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training\n",
    "\n",
    "\n",
    "\n",
    "**Example:**\n",
    "- We use the encoder from tiktoken (tiktoken.get_encoding(\"gpt2\")) has a vocabulary size of 50,257. Therefore we create the token embedding layer with the vocabulary size of 50,257. \n",
    "- Suppose that we want the **output dimension** of the embedding layer to be 256.\n",
    "- As a result, after passing the token embedding layers, every token_id will be mapped to a vector of size 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e457a18b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:17.371498Z",
     "iopub.status.busy": "2024-08-23T19:01:17.371052Z",
     "iopub.status.idle": "2024-08-23T19:01:17.557247Z",
     "shell.execute_reply": "2024-08-23T19:01:17.555815Z"
    },
    "papermill": {
     "duration": 0.203355,
     "end_time": "2024-08-23T19:01:17.560058",
     "exception": false,
     "start_time": "2024-08-23T19:01:17.356703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape: torch.Size([8, 4])\n",
      "Output shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\", inputs.shape)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"Output shape:\", token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429c1bd",
   "metadata": {
    "papermill": {
     "duration": 0.013823,
     "end_time": "2024-08-23T19:01:17.591150",
     "exception": false,
     "start_time": "2024-08-23T19:01:17.577327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### b. Posistion embedding: encoding word positions\n",
    "- Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence:\n",
    "- Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model:\n",
    "\n",
    "\n",
    "In the following code, we created the position embeding with the max length of sequence is 4. We will pass the positions of the tokens to the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c32a667b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:17.619198Z",
     "iopub.status.busy": "2024-08-23T19:01:17.618758Z",
     "iopub.status.idle": "2024-08-23T19:01:17.633506Z",
     "shell.execute_reply": "2024-08-23T19:01:17.632279Z"
    },
    "papermill": {
     "duration": 0.031927,
     "end_time": "2024-08-23T19:01:17.636183",
     "exception": false,
     "start_time": "2024-08-23T19:01:17.604256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input of position embedding layer tensor([0, 1, 2, 3])\n",
      "Output shape torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print('Input of position embedding layer', torch.arange(max_length))\n",
    "print('Output shape', pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f060c",
   "metadata": {
    "papermill": {
     "duration": 0.013177,
     "end_time": "2024-08-23T19:01:17.662988",
     "exception": false,
     "start_time": "2024-08-23T19:01:17.649811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### c. Combine both token embedding and position embedding.\n",
    "\n",
    "Finally the input to the GPT model is the sum of token embeddings and position embeddings. The whole process can be illustrated in the following picture:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5724029d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:17.695622Z",
     "iopub.status.busy": "2024-08-23T19:01:17.694767Z",
     "iopub.status.idle": "2024-08-23T19:01:17.710033Z",
     "shell.execute_reply": "2024-08-23T19:01:17.708566Z"
    },
    "papermill": {
     "duration": 0.037116,
     "end_time": "2024-08-23T19:01:17.713548",
     "exception": false,
     "start_time": "2024-08-23T19:01:17.676432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3b4eb",
   "metadata": {
    "papermill": {
     "duration": 0.014956,
     "end_time": "2024-08-23T19:01:17.747145",
     "exception": false,
     "start_time": "2024-08-23T19:01:17.732189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Activity:\n",
    "Try to encode and decode the string \"**What is the opposite word of hot?**\" using the tokenizer **SimpleTokenizerV2** and **tiktoken.get_encoding(\"gpt2\")**. \n",
    "- Print then token ids of the string. \n",
    "- You may also want to decode the ids to get the orginal text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "087dd74b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T19:01:17.781153Z",
     "iopub.status.busy": "2024-08-23T19:01:17.780720Z",
     "iopub.status.idle": "2024-08-23T19:01:17.790064Z",
     "shell.execute_reply": "2024-08-23T19:01:17.788942Z"
    },
    "papermill": {
     "duration": 0.02726,
     "end_time": "2024-08-23T19:01:17.792639",
     "exception": false,
     "start_time": "2024-08-23T19:01:17.765379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding...\n",
      "[109, 584, 988, 1131, 1116, 722, 1131, 10]\n",
      "[2061, 318, 262, 6697, 1573, 286, 3024, 30]\n",
      "\n",
      "Decoding...\n",
      "What is the <|unk|> word of <|unk|>?\n",
      "What is the opposite word of hot?\n"
     ]
    }
   ],
   "source": [
    "tokenizer_1 = SimpleTokenizerV2(vocab)\n",
    "tokenizer_2 = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "act_text = \"What is the opposite word of hot?\"\n",
    "\n",
    "print(\"Encoding...\")\n",
    "encoded_1 = tokenizer_1.encode(act_text)\n",
    "encoded_2 = tokenizer_2.encode(act_text)\n",
    "\n",
    "print(encoded_1)\n",
    "print(encoded_2)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Decoding...\")\n",
    "decoded_1 = tokenizer_1.decode(encoded_1)\n",
    "decoded_2 = tokenizer_2.decode(encoded_2)\n",
    "\n",
    "print(decoded_1)\n",
    "print(decoded_2)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.798528,
   "end_time": "2024-08-23T19:01:19.030496",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-23T19:00:48.231968",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
