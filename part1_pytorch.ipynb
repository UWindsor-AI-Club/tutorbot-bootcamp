{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is PyTorch?\n",
    "Two main use cases:\n",
    "1. Leverage numpy arrays on the GPU\n",
    "2. Deep learning research platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Intro to Tensors\n",
    "- Tensors are a fundamental data structure similiar to arrays and matrices\n",
    "- Efficient for math operations on large data \n",
    "- The data structure used in pytorch models\n",
    "- Similiar to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:  tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Creating a Tensor from a Numpy Array\n",
    "ndarray = np.array([1, 2, 3])\n",
    "t = torch.from_numpy(ndarray)\n",
    "print(\"Tensor: \", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Attributes of a Tensor:\n",
    "1. shape: size of the tensor\n",
    "2. data type: type of data stored in the tensor\n",
    "3. device: the device the tensor is stored in (e.g. cpu, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.shape)\n",
    "print(t.dtype)\n",
    "print(t.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([0, 1, 2])\n",
      "First column:  tensor([0, 3])\n",
      "Transpose: \n",
      " tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "Multiplication: \n",
      " tensor([[15, 17, 19],\n",
      "        [48, 53, 58]])\n",
      "Same thing... \n",
      " tensor([[15, 17, 19],\n",
      "        [48, 53, 58]])\n"
     ]
    }
   ],
   "source": [
    "ndarray = np.array([[0, 1, 2], [3, 4, 5]])\n",
    "tensor_2 = torch.from_numpy(ndarray)\n",
    "\n",
    "# Index first row:\n",
    "print(\"First row: \", tensor_2[0])\n",
    "\n",
    "# Index first column:\n",
    "print(\"First column: \", tensor_2[:, 0])\n",
    "\n",
    "# Transpose the tensor\n",
    "print(\"Transpose: \\n\", tensor_2.T)\n",
    "\n",
    "# Multiply the Tensor: (Matrix Multiplication)\n",
    "ndarray = np.array([[2, 2, 2], [3, 3, 3], [6, 7, 8]])\n",
    "tensor_3 = torch.from_numpy(ndarray)\n",
    "print(\"Multiplication: \\n\", torch.matmul(tensor_2, tensor_3))\n",
    "\n",
    "# You can also use the @ operator to perform matrix multiplication:\n",
    "print(\"Same thing... \\n\", tensor_2 @ tensor_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1: \n",
    "- Complete `matrix_multiplication` function below\n",
    "- Input: two numpy arrays\n",
    "- Return: the matrix multiplication as a tensor IF the two numpy arrays can be multiplied\n",
    "    - returns null if they can't be multiplied\n",
    "\n",
    "> Recall: For matrix mulitplication of A and B, the number of cols in A must equal the number of rows in B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiplication(A: np.ndarray, B: np.ndarray) -> torch.Tensor:\n",
    "    # Write Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "# Test Cases\n",
    "# Test Case 1: Standard multiplication\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "expected_output = torch.tensor([[19, 22], [43, 50]])\n",
    "output = matrix_multiplication(A, B)\n",
    "assert torch.equal(output, expected_output), f\"Test Case 1 Failed: {output}\"\n",
    "\n",
    "# Test Case 2: Different dimensions but valid for multiplication\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "B = np.array([[7, 8], [9, 10], [11, 12]])\n",
    "expected_output = torch.tensor([[58, 64], [139, 154]])\n",
    "output = matrix_multiplication(A, B)\n",
    "assert torch.equal(output, expected_output), f\"Test Case 2 Failed: {output}\"\n",
    "\n",
    "# Test Case 3: Incompatible dimensions\n",
    "A = np.array([[1, 2]])\n",
    "B = np.array([[3, 4], [5, 6], [7, 8]])\n",
    "output = matrix_multiplication(A, B)\n",
    "assert output is None, f\"Test Case 3 Failed: {output}\"\n",
    "\n",
    "print(\"All test cases passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Automatic Differentiation in PyTorch\n",
    "- this module in pytorch will allow the automatifc calculation of gradients\n",
    "- What are gradients?\n",
    "    - gradients represent the rate of change of functions with respect to params\n",
    "    - helps identify the difference between predicted outputs and actual labels\n",
    "- pytorch will automatically create the function needed for backward propagation in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor z: tensor(31., grad_fn=<AddBackward0>)\n",
      "Gradient of x: tensor(4.)\n",
      "Gradient of y: tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "# Example 1:\n",
    "\n",
    "# Define tensors with requires_grad=True to track computation history\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    " \n",
    "# Perform a computation\n",
    "z = x**2 + y**3\n",
    "print(\"Output tensor z:\", z)\n",
    " \n",
    "# Compute gradients - run backpropagation\n",
    "z.backward()\n",
    "\n",
    "# Print the gradients\n",
    "print(\"Gradient of x:\", x.grad)\n",
    "print(\"Gradient of y:\", y.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[ 0.5573, -0.4756,  0.1310],\n",
      "        [-0.1495, -0.5322,  0.2433]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([-0.3484, -0.1480], requires_grad=True)\n",
      "loss:  2.1264235973358154\n",
      "dL/dw:  tensor([[ 1.3172, -0.4402,  0.4978],\n",
      "        [ 0.2199, -0.7606,  0.8396]])\n",
      "dL/db:  tensor([-0.4920, -0.3344])\n",
      "loss after 1 step optimization:  2.088127613067627\n"
     ]
    }
   ],
   "source": [
    "# Example 2 (Optional):\n",
    "# This example demonstrates how gradients can be put to use:\n",
    "\n",
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)\n",
    "\n",
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# 1-step gradient descent.\n",
    "optimizer.step()\n",
    "\n",
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
